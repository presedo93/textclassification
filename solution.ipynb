{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a candidate to develop a solution that is capable to classify provided texts in one of **four** classes.\n",
    " \n",
    "You may find the dataset in the **data** folder:\n",
    "- train.csv contains training dataset. There are four columns in this file:\n",
    "    - id - column with unique identifier of each data sample\n",
    "    - category - target variable\n",
    "    - title - document title\n",
    "    - description - document text\n",
    "- test.csv contains test dataset and all the columns are the same except category as it is unknown and should be predicted.\n",
    "- sample_submission.csv - an example of how resulting submission shoul look like.\n",
    "\n",
    "Your model should give as an output a probability of each sample belonging to each class.\n",
    "\n",
    "To submit your solution put this **solution.ipynb** file and generated **submission.csv** in a **zip** file.\n",
    "\n",
    "We are interested to see how candidate implements his/her typical pipeline to solve machine learning problems starting with a dataset containing both data and target variable.\n",
    "\n",
    "We **do not** expect a state-of-the-art solution here, rather a code that demonstrates candidate's understanding of crucial parts in ML models development. However, it would be a plus to see a brief description on how to get to the near-state-of-the-art solution in conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# add needed libraries here\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download NLTK stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 32\n",
    "max_length_a = 200\n",
    "max_length_t = 50\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code in this and the following blocks\n",
    "df_train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just check the head of the dataframe\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the mean lengths of the titles and the descriptions\n",
    "# make sense with the values written in the hyperparameters cell\n",
    "print('Mean Length of titles:', round(df_train.title.apply(len).mean(), 2))\n",
    "print('Mean Length of articles:', round(df_train.description.apply(len).mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    '''Function to remove English stopwords from a Pandas Series.'''\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and create train data\n",
    "train_titles = df_train.title.apply(remove_stopwords)[:].to_numpy()\n",
    "train_articles = df_train.description.apply(remove_stopwords)[:].to_numpy()\n",
    "train_targets = df_train.category[:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.concatenate((train_titles, train_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size,\n",
    "                                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{\"}~\\t\\n',\n",
    "                                               lower=True,\n",
    "                                               oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print most common words\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to sequences\n",
    "train_title_s = tokenizer.texts_to_sequences(train_titles)\n",
    "train_article_s = tokenizer.texts_to_sequences(train_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "train_title_p = pad_sequences(train_title_s, \n",
    "                              maxlen=max_length_t, \n",
    "                              padding='post',\n",
    "                              truncating='post')\n",
    "\n",
    "train_article_p = pad_sequences(train_article_s, \n",
    "                              maxlen=max_length_a, \n",
    "                              padding='post',\n",
    "                              truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train example\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_article(train_article_p[10]))\n",
    "print('***')\n",
    "print(train_articles[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_model():\n",
    "    '''2 inputs model that performs Embedding and Bidirectional GRU'''\n",
    "    articles = tf.keras.layers.Input(shape=(vocab_size,))\n",
    "    titles = tf.keras.layers.Input(shape=(vocab_size,))\n",
    "    \n",
    "    a = tf.keras.layers.Embedding(vocab_size, embedding_dim)(articles)\n",
    "    a = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embedding_dim))(a)\n",
    "    a = tf.keras.layers.Dropout(0.5)(a)\n",
    "    a = tf.keras.layers.Dense(embedding_dim / 2, activation='relu')(a)\n",
    "    \n",
    "    t = tf.keras.layers.Embedding(vocab_size, embedding_dim)(titles)\n",
    "    t = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(embedding_dim))(t)\n",
    "    t = tf.keras.layers.Dropout(0.5)(t)\n",
    "    t = tf.keras.layers.Dense(embedding_dim / 2, activation='relu')(t)\n",
    "    \n",
    "    z = tf.keras.layers.concatenate([a, t])\n",
    "    outputs = tf.keras.layers.Dense(4, activation='softmax')(z)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=(articles, titles), outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = nlp_model()\n",
    "\n",
    "# Compile the model with Adam optimizer and Sparse Categorical Crossentropy loss\n",
    "adam = tf.keras.optimizers.Adam(lr=3e-4)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Use of Reduce Learning Rate on Plateau and EarlyStopping\n",
    "lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "# Start training process\n",
    "history = model.fit((train_article_p, train_title_p), train_targets, epochs=num_epochs,\n",
    "                   validation_split=0.1, shuffle=True,\n",
    "                   callbacks=[lr_plateau, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss metrics\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_' + string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend(['train_' + string, 'val_' + string])\n",
    "  plt.show()\n",
    "    \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_titles = df_test.title.apply(remove_stopwords)[:].to_numpy()\n",
    "test_articles = df_test.description.apply(remove_stopwords)[:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title_s = tokenizer.texts_to_sequences(test_titles)\n",
    "test_article_s = tokenizer.texts_to_sequences(test_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title_p = pad_sequences(test_title_s, \n",
    "                              maxlen=max_length_t, \n",
    "                              padding='post',\n",
    "                              truncating='post')\n",
    "\n",
    "test_article_p = pad_sequences(test_article_s, \n",
    "                              maxlen=max_length_a, \n",
    "                              padding='post',\n",
    "                              truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict((test_article_p, test_title_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the following code to generate a submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['category_0'] = predictions[:, 0]\n",
    "submission['category_1'] = predictions[:, 1]\n",
    "submission['category_2'] = predictions[:, 2]\n",
    "submission['category_3'] = predictions[:, 3]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colnclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few words about your solution here. \n",
    "- My approach has been to use a model that has two inputs. One input is the titles and the second one is the description of the articles. Both inputs go through the same layers:\n",
    "    - An Embedding layer to aproach for representing words using a dense vector.\n",
    "    - A Bidirectional GRU layer to learn \"throught time\" from the titles and descriptions.\n",
    "    - A Dropout layer to perform some regularization.\n",
    "    - A Dense layer with a ReLU activation.\n",
    "- As a final stage the model concatenates both branches and makes use of a final Dense layer with a softmax activation.    \n",
    "\n",
    "What could be improved? \n",
    " - The model seems to be overfitting. Even though the training data is performing better each iteration, the validation data moves around the same values. Some actions that could be taken:\n",
    "     - Apply a more aggressive regularitazion to the network (or other methods such as L2 regularization).\n",
    "     - Reduce the networks capacity, to check if that solves the networks overfitting.\n",
    "     - Search for a better configuration of the hyperparameters.\n",
    "     \n",
    "What approaches may work as well for this problem? \n",
    "- Make use of Conv1D layers. Conv layers are also commonly used in nlp and forecasting tasks.\n",
    "- Create a model full of Dense Layers. For basic nlp tasks, some Dense layers architectures also perform quite well.\n",
    "\n",
    "What would you implement if you have had more time for this task?\n",
    "- Nowadays, Attention models have hit pretty hard. They are replacing LSTM/GRU models in state-of-the-art models. So, it could have been a great approach for this problem to try, for example, the Transformer neural network architecture.\n",
    "\n",
    "Feel free to write anything you think is relevant to this task :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
